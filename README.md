NLP里的attention机制
===================


## attention机制解读

https://zhuanlan.zhihu.com/p/40920384

https://zhuanlan.zhihu.com/p/31547842

https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/

https://mp.weixin.qq.com/s/5D8ZjMjX7T-sHrdWIODQMA



## 代码

https://zhuanlan.zhihu.com/p/27769667

https://github.com/tensorflow/nmt

https://github.com/princewen/tensorflow_practice/tree/master/nlp/chat_bot_seq2seq_attention

http://nlp.seas.harvard.edu/2018/04/03/attention.html

https://github.com/harvardnlp/annotated-transformer




